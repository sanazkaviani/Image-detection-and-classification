{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkZLWXVbfDDF"
      },
      "source": [
        "### Kaggle competition_ overfitters\r\n",
        "Sanaz Kaviani, sanaz.kaviani@umontreal.ca, marticule: 2111567 \r\n",
        "Mersede Mokri, mersede.mokri@umontreal.ca, marticule: 2111556\r\n",
        "Hamed Naseri, hamed.naseri@polymtl.ca, marticule: 2051414\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htnP8IKcWpqo"
      },
      "source": [
        "### In the following part, the required standard libraries are imported, and the preprocessing function is modeled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHMPYANGNwOq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb9b40b-6b22-4486-96ed-391199aa6a0c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # used to Disable Tensorflow debugging informatio\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import measure, morphology\n",
        "import keras\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR_Y-AzOabrO"
      },
      "source": [
        "## Pre-Processing \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6D8zqB6th4K",
        "outputId": "d2082003-ed2a-4a04-82c1-4259dfe57b77"
      },
      "source": [
        "\"\"\" Pre-Processing \"\"\"\n",
        "def PreProcessing_1(dataSample, filteredImgSize, binarizingTH):\n",
        "    dataSize = dataSample.shape[0]               #Number of Examples\n",
        "    NumPixels = dataSample[0][1].shape[0]       #Original Feature Size which is imageLength*imageWidth\n",
        "    originalImgSize = np.sqrt(NumPixels).astype(int)   #Original Image Size\n",
        "    \"\"\" Initialization \"\"\"\n",
        "    filteredImgCenter = int(filteredImgSize/2)   #Center of the Filtered Image (25 here)\n",
        "    featureSize = filteredImgSize**2\n",
        "    processedImageSet = np.zeros((dataSize, featureSize))\n",
        "    \"\"\" First Train Set: to find Vocabs of SIFT, SURF and anyother feature \"\"\"\n",
        "    for i in range(dataSize):\n",
        "        if (i+1)%1000 == 0:\n",
        "            print(i , ' of data have been processed')\n",
        "        originalImg = dataSample[i,1].reshape(originalImgSize,originalImgSize)\n",
        "        originalImg[originalImg > 255] = 255\n",
        "        binarizedImg = originalImg > binarizingTH  #this needs to be tunned\n",
        "        origImgSegments= measure.label(binarizedImg, background = 0)\n",
        "        mostCommonLabel = Counter(origImgSegments.flatten()).most_common(3)\n",
        "        filterMask = (origImgSegments == mostCommonLabel[1][0]) #+ (origImgSegments ==mostCommonLabel[2][0])\n",
        "        filteredImg = filterMask * originalImg\n",
        "        regionImg = measure.regionprops(filterMask.astype(int))[0]\n",
        "        originalImgCenter = [int(regionImg.bbox[0]+((regionImg.bbox[2]-regionImg.bbox[0])/2)), int(regionImg.bbox[1]+((regionImg.bbox[3]-regionImg.bbox[1])/2))]\n",
        "        deltaX = regionImg.bbox[2]-regionImg.bbox[0]\n",
        "        deltaY = regionImg.bbox[3]-regionImg.bbox[1]\n",
        "        deltaX = int( min(1.5*deltaX , filteredImgSize))\n",
        "        deltaX += deltaX%2\n",
        "        deltaY = int( min(1.5*deltaY , filteredImgSize))\n",
        "        deltaY += deltaY%2\n",
        "        grabbedImg = []\n",
        "        grabbedImg = filteredImg[max(0,originalImgCenter[0]-int(deltaX/2)):min(originalImgSize-1,originalImgCenter[0]+int(deltaX/2)), max(0,originalImgCenter[1]-int(deltaY/2)):min(originalImgSize-1,originalImgCenter[1]+int(deltaY/2))]\n",
        "        tmpXsize = grabbedImg.shape[0]\n",
        "        tmpYsize = grabbedImg.shape[1]\n",
        "        tmpImg = np.zeros((filteredImgSize , filteredImgSize))\n",
        "        tmpImg[filteredImgCenter-int(tmpXsize/2) : filteredImgCenter+int(tmpXsize/2)+tmpXsize%2, filteredImgCenter-int(tmpYsize/2): filteredImgCenter+int(tmpYsize/2)+tmpYsize%2] = grabbedImg\n",
        "        tmpImgSegments= measure.label(tmpImg>binarizingTH, background = 0)\n",
        "        mostCommonLabel = Counter(tmpImgSegments.flatten()).most_common()\n",
        "        finalMask = morphology.remove_small_objects(tmpImg>binarizingTH, min(40, mostCommonLabel[1][1]-1), connectivity=2)\n",
        "        tmpImg = tmpImg*finalMask\n",
        "        outImg = tmpImg / np.max(tmpImg)\n",
        "        processedImageSet[i,:] = outImg.flatten()\n",
        "    return processedImageSet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX5ehIRaaq0f"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7JQLDvOapEZ"
      },
      "source": [
        "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
        "\n",
        "\n",
        "\"\"\"                    Loading Train and Test Data:                         \"\"\"\n",
        "dataTrain = np.load('/content/drive/MyDrive/train_images.npy', encoding='latin1', allow_pickle=True)\n",
        "dataTest  = np.load('/content/drive/MyDrive/test_images.npy',  encoding = 'latin1', allow_pickle=True)\n",
        "\n",
        "\"\"\"  Extracting Train & Test Sets Sizes and Original Feature & Image Sizes  \"\"\"\n",
        "trainSize = dataTrain.shape[0]  # Number of Training Examples\n",
        "testSize  = dataTest.shape[0]                #Number of Testing Examples\n",
        "\n",
        "\"\"\"    Loading Labels of Train Data & Converting Words to Numeric Labels    \"\"\"\n",
        "myCSV = np.genfromtxt('/content/drive/MyDrive/train_labels.csv', delimiter=',', dtype='str')\n",
        "    # myCSV = np.genfromtxt('./all/train_labels.csv', delimiter=',', dtype = 'str')\n",
        "trainLabelWords = myCSV[1:, 1]  # Training labels: Words\n",
        "uniqueLabelWords = np.unique(trainLabelWords)  # Unique Labels\n",
        "trainLabel = np.zeros((trainSize, 1))  # Training Labels: Numerics\n",
        "NumberLabel = uniqueLabelWords.shape[0]\n",
        "refLabel = np.zeros((NumberLabel, 2))\n",
        "for i in range(NumberLabel):\n",
        "   trainLabel[trainLabelWords == uniqueLabelWords[i], 0] = i\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hel3IElsbSw0"
      },
      "source": [
        "### In the following part, data are initialized and preprocessing process is performed to ehnace the model's accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7EJwuteac8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "644351c8-8380-470f-85e5-eb344ca2b36a"
      },
      "source": [
        "\"\"\" Parameter Initialization \"\"\"\n",
        "filteredImgSize = 40\n",
        "binarizingTH = 8  # for first segmentation\n",
        "\n",
        "processedImgTrain1 = PreProcessing_1(dataTrain, filteredImgSize, binarizingTH)\n",
        "processedImgTest1 = PreProcessing_1(dataTest, filteredImgSize, binarizingTH)\n",
        "\n",
        "binarizingTH = 0.01  # range (0,1)\n",
        "featMatrixTrain = 1 * (processedImgTrain1 > binarizingTH)\n",
        "featMatrixTest  = 1 * (processedImgTest1>binarizingTH)\n",
        "featMatrixTrain = featMatrixTrain.astype('float32')\n",
        "featMatrixTest = featMatrixTest.astype('float32')\n",
        "\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "\n",
        "\"\"\"  Extracting Train & Test Sets Sizes and Original Feature & Image Sizes  \"\"\"\n",
        "trainSize = featMatrixTrain.shape[0]               #Number of Training Examples\n",
        "testSize  = featMatrixTest.shape[0]                #Number of Testing Examples\n",
        "featureSize = featMatrixTrain.shape[1]             #Number of Features\n",
        "\n",
        "\n",
        "nTrain = int(0.9*trainSize)\n",
        "idxTrainValid = np.random.choice(trainSize, [trainSize,1],replace = False)\n",
        "\n",
        "t_train = trainLabel[idxTrainValid[:nTrain],0]\n",
        "t_valid = trainLabel[idxTrainValid[nTrain:],0]\n",
        "X_train = featMatrixTrain[idxTrainValid[:nTrain,0],:]\n",
        "X_valid = featMatrixTrain[idxTrainValid[nTrain:,0],:]\n",
        "X_test = featMatrixTest\n",
        "\n",
        "X_train = (X_train.reshape(X_train.shape[0], 1,filteredImgSize, filteredImgSize))\n",
        "#X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_valid = (X_valid.reshape(X_valid.shape[0], 1,filteredImgSize, filteredImgSize))\n",
        "#X_valid = np.expand_dims(X_valid, axis=-1)\n",
        "X_test = (X_test.reshape(X_test.shape[0], 1,filteredImgSize, filteredImgSize))\n",
        "#X_test = np.expand_dims(X_test, axis=-1)\n",
        "    #one-hot encode target column\n",
        "y_train = to_categorical(t_train)\n",
        "y_valid = to_categorical(t_valid)\n",
        "\n",
        "num_classes = y_valid.shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "999  of data have been processed\n",
            "1999  of data have been processed\n",
            "2999  of data have been processed\n",
            "3999  of data have been processed\n",
            "4999  of data have been processed\n",
            "5999  of data have been processed\n",
            "6999  of data have been processed\n",
            "7999  of data have been processed\n",
            "8999  of data have been processed\n",
            "9999  of data have been processed\n",
            "999  of data have been processed\n",
            "1999  of data have been processed\n",
            "2999  of data have been processed\n",
            "3999  of data have been processed\n",
            "4999  of data have been processed\n",
            "5999  of data have been processed\n",
            "6999  of data have been processed\n",
            "7999  of data have been processed\n",
            "8999  of data have been processed\n",
            "9999  of data have been processed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z9ix-Jbbw2E"
      },
      "source": [
        "## Convolutional Neural Network is defined in the following part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erq7x8YqfY-t"
      },
      "source": [
        "def CNN_model(filteredImgSize=40):\n",
        "    model = Sequential([\n",
        "        # First two convolutional layers\n",
        "        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(1, filteredImgSize, filteredImgSize)),\n",
        "        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        # normalization layer\n",
        "        BatchNormalization(),\n",
        "        # pooling layer\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        # add regularization\n",
        "        Dropout(0.25),\n",
        "        # Second two convolutional layers\n",
        "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        # normalization layer\n",
        "        BatchNormalization(),\n",
        "        # pooling layer\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        # add regularization\n",
        "        Dropout(0.25),\n",
        "\n",
        "        Flatten(),\n",
        "\n",
        "        # FC layer\n",
        "        Dense(1024, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(1024, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(1024, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(31, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                  optimizer=keras.optimizers.Adam(lr=0.0004),\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVlqu8mIb8TS"
      },
      "source": [
        "# CNN model is run, and its accuracy for training and validation data is reported for each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cAzOrJW36Ci"
      },
      "source": [
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "modelBest=CNN_model()\n",
        "\n",
        "batch_size=35\n",
        "epoch_aug1=700\n",
        "\n",
        "#earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=30, verbose=0, mode='min')\n",
        "\n",
        "gen = ImageDataGenerator(rotation_range=15, width_shift_range=0.1, shear_range=0.3, \n",
        "                         height_shift_range=0.1, zoom_range=0.1)\n",
        "\n",
        "\n",
        "\n",
        "batches = gen.flow(X_train, y_train, batch_size=batch_size)\n",
        "val_batches = gen.flow(X_valid, y_valid, batch_size=batch_size)\n",
        "     \n",
        "\n",
        "results=modelBest.fit_generator(batches, steps_per_epoch=X_train.shape[0] // batch_size, epochs=epoch_aug1,\n",
        "                                validation_data=val_batches, validation_steps=X_valid.shape[0] // batch_size,\n",
        "                                use_multiprocessing=False)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(results.history['accuracy'], label=\"Train\")\n",
        "plt.plot(results.history['val_accuracy'], label=\"Validation\")\n",
        "plt.legend(fontsize=25)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# The final accuracy of CNN for training and validation data is reported in the following part.\n",
        "scores = modelBest.evaluate(X_train, y_train, verbose=1)\n",
        "print(\"Large CNN Train Error: %.2f%%\" % (100-scores[1]*100))\n",
        "scores = modelBest.evaluate(X_valid, y_valid, verbose=1)\n",
        "print(\"Large CNN Valid Error: %.2f%%\" % (100-scores[1]*100))\n",
        "\n",
        "\n",
        "\n",
        "#predict images in the test set\n",
        "y_test_CNN = modelBest.predict(X_test)\n",
        "t_test_CNN = np.argmax(y_test_CNN,axis=1)\n",
        "\n",
        "testLabelCNN = np.zeros((testSize,2)).astype('str')\n",
        "\n",
        "# map the predict result to classes name\n",
        "for i in range(NumberLabel):\n",
        "  testLabelCNN[t_test_CNN == i,1] = uniqueLabelWords[i]\n",
        "\n",
        "testLabelCNN[:,0]=range(10000)\n",
        "\n",
        "# The testing data labels are predicted in the following part, and the predicted labels are saved in a csv file in order to be uploaded in Kaggle website.  \n",
        "test=pd.DataFrame(testLabelCNN,columns=['Id','Category']).set_index('Id')\n",
        "test.to_csv('7810.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}